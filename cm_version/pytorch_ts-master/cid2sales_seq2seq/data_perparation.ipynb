{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "import pdb\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python3.6\"\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"be\")\n",
    "    .enableHiveSupport()\n",
    "    .config(\"spark.executor.instances\", \"300\")\n",
    "    .config(\"spark.executor.memory\",\"16g\")\n",
    "    .config(\"spark.executor.cores\",\"1\")\n",
    "    .config(\"spark.driver.cores\", \"8\")\n",
    "    .config(\"spark.driver.memory\",\"8g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"36000\")\n",
    "    .config(\"spark.sql.shuffle.partitions\",\"3000\")\n",
    "    .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\n",
    "    .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\n",
    "    .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "    .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sql(\"set hive.exec.orc.split.strategy=ETL\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql(\"set hive.exec.max.dynamic.partitions=3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预售预约数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_max_dt(df_name):\n",
    "    df = spark.sql(\"show partitions {0}\".format(df_name)).toPandas()\n",
    "    try:\n",
    "        dt_list = df.partition.apply(lambda x: [ele for ele in x.split('/') if ele[:3] == 'dt='][0][3:])\n",
    "        return sorted([ele for ele in dt_list if ele[0].isnumeric()])[-1]\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================= 预售数据 ============================= #\n",
    "# 预售数据来自订单表\n",
    "\n",
    "# 按照下单日期口径统计销量\n",
    "qtty_by_order_dt = spark.sql('''\n",
    "                                select \n",
    "                                    item_sku_id, \n",
    "                                    dt, \n",
    "                                    sum(sale_qtty) as qtty_by_order_dt \n",
    "                                from \n",
    "                                    dev.dev_pathway_salessource_skudt_det\n",
    "                                where \n",
    "                                    dt_ord is not null \n",
    "                                    and sale_qtty between 1 and 5 \n",
    "                                    and business_type = 0 \n",
    "                                    and sale_ord_type_cd = 0  \n",
    "                                group by \n",
    "                                    item_sku_id, \n",
    "                                    dt\n",
    "                             ''')\n",
    "\n",
    "start_dt, end_dt = '2018-01-01', extract_max_dt('dev.dev_pathway_salessource_skudt_det')\n",
    "\n",
    "# 仅选取最后一天上柜的SKU\n",
    "sku_range = spark.sql('''\n",
    "                          select \n",
    "                              sku_id as item_sku_id \n",
    "                          from \n",
    "                              dev.self_sku_det_da \n",
    "                          where \n",
    "                              dt = '{0}'\n",
    "                              and status = 1\n",
    "                      '''.format(end_dt)\n",
    "                     )\n",
    "\n",
    "\n",
    "# SKU上柜日期\n",
    "sku_status = spark.sql('''\n",
    "                           select \n",
    "                               sku_id as item_sku_id, \n",
    "                               dt \n",
    "                           from \n",
    "                               dev.self_sku_det_da \n",
    "                           where \n",
    "                               (dt between '{start_sql}' and '{end_sql}')\n",
    "                               and sku_type = 1 \n",
    "                               and status = 1\n",
    "                       '''.format(start_sql = start_dt, end_sql = end_dt))\n",
    "\n",
    "# 从订单表中拿到预售状态\n",
    "presale_raw_df = spark.sql('''\n",
    "                                 select distinct \n",
    "                                     item_sku_id, \n",
    "                                     sale_ord_dt, \n",
    "                                     substr(check_account_tm, 1, 10) as checkout_dt, \n",
    "                                     substr(ord_flag, 44, 1) as ord_flag\n",
    "                                 from \n",
    "                                     dev.all_sku_order_det\n",
    "                                 where \n",
    "                                     (dt between '{0}' and '{1}') \n",
    "                                     and sku_type = 1 \n",
    "                                     and sale_ord_valid_flag = 1 \n",
    "                                     and sale_ord_type_cd = 0 \n",
    "                           '''.format(start_dt, end_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_range = sku_range.join(qtty_by_order_dt.select('item_sku_id').distinct(), 'item_sku_id', 'inner')\n",
    "sku_status = sku_status.join(sku_range, 'item_sku_id', 'inner')\n",
    "presale_raw_df = presale_raw_df.join(sku_range,'item_sku_id','left_semi') \\\n",
    "                               .filter(F.col('ord_flag') != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "presale_flag = presale_raw_df.filter(F.col('ord_flag') == 1) \\\n",
    "                             .select('item_sku_id', 'sale_ord_dt') \\\n",
    "                             .withColumnRenamed('sale_ord_dt', 'dt') \\\n",
    "                             .withColumn('presale_flag', F.lit(1))\n",
    "\n",
    "presale_pay_flag = presale_raw_df.filter(F.col('ord_flag') == 1) \\\n",
    "                                 .select('item_sku_id', 'checkout_dt') \\\n",
    "                                 .withColumnRenamed('checkout_dt', 'dt') \\\n",
    "                                 .withColumn('presale_pay_flag', F.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "presale_df = sku_status.join(qtty_by_order_dt, ['item_sku_id','dt'], 'left') \\\n",
    "                       .join(presale_flag.distinct(), ['item_sku_id','dt'], 'left') \\\n",
    "                       .join(presale_pay_flag.distinct(), ['item_sku_id','dt'], 'left').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8222330"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presale_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================= 预售数据 ============================= #\n",
    "# 预约数据来自fdm底层表\n",
    "\n",
    "start_date = '2017-12-15'\n",
    "last_date = str(datetime.now() + timedelta(days=-1))[:10]\n",
    "date_list = list(pd.date_range(start_date, last_date).astype(str))\n",
    "date_df = spark.createDataFrame(pd.DataFrame(date_list, columns=['dt']))\n",
    "\n",
    "sku_cid = spark.sql('''\n",
    "                        select \n",
    "                            sku_id as item_sku_id, \n",
    "                            cid1, \n",
    "                            cid2, \n",
    "                            cid3, \n",
    "                            brand_code \n",
    "                        from \n",
    "                            dev.self_sku_det_da \n",
    "                        where \n",
    "                            dt = '{0}' and \n",
    "                            (cid1 = 737) \n",
    "                            and sku_status_cd = 3001 and sku_type =1\n",
    "                    '''.format(last_date)\n",
    "                    )\n",
    "\n",
    "# 获取预约起始时间和预约付款（抢购）起始时间\n",
    "booking_range = spark.sql('''\n",
    "                            select skuid as item_sku_id, \n",
    "                                substr(start_time, 1, 10) as start_time, \n",
    "                                substr(end_time, 1, 10) as end_time, \n",
    "                                substr(panicbuying_start_time, 1, 10) as panicbuying_start_time,\n",
    "                                substr(panicbuying_end_time, 1, 10) as panicbuying_end_time \n",
    "                            from \n",
    "                                fdm.fdm_pre_sell_schema_pre_sell_info_chain \n",
    "                            where \n",
    "                                dp = 'ACTIVE'\n",
    "                                and substr(end_time,1,10) >= '{}'\n",
    "                           '''.format(start_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_range = booking_range.join(sku_cid.select('item_sku_id'), 'item_sku_id','inner')\n",
    "booking_flag = booking_range.join(date_df.select('dt'),\n",
    "                                  on=date_df.dt.between(booking_range.start_time, booking_range.end_time)\n",
    "                                 ).select('item_sku_id','dt').distinct()\n",
    "booking_flag = booking_flag.withColumn('booking_flag',F.lit(1)) \\\n",
    "                           .withColumn('booking_pay_flag',F.lit(0))\n",
    "booking_pay_flag = booking_range.join(date_df.select('dt'),\n",
    "                                      on=date_df.dt.between(booking_range.panicbuying_start_time, \n",
    "                                                            booking_range.panicbuying_end_time)\n",
    "                                     ).select('item_sku_id','dt').distinct()\n",
    "booking_pay_flag = booking_pay_flag.withColumn('bookinging_flag',F.lit(0)) \\\n",
    "                                   .withColumn('bookinging_pay_flag',F.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_df = booking_flag.unionAll(booking_pay_flag) \\\n",
    "                       .groupby(['item_sku_id', 'dt']).agg(F.sum(F.col('booking_flag')).alias('booking_flag'),\n",
    "                                                           F.sum(F.col('booking_pay_flag')).alias('booking_pay_flag')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "865646"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booking_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "presale_df = presale_df.fillna(0, subset=['presale_flag', 'presale_pay_flag'])\n",
    "presale_booking_df = presale_df.join(booking_df, on=['item_sku_id', 'dt'], how='left') \\\n",
    "                               .fillna(0, subset=['booking_flag', 'booking_pay_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "presale_booking_df.select('item_sku_id', 'qtty_by_order_dt', 'presale_flag', 'presale_pay_flag',\n",
    "                          'booking_flag', 'booking_pay_flag', 'dt') \\\n",
    "                  .repartition('dt').write.insertInto('app.cid2_794_daily_sale', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_max_dt(df_name):\n",
    "    df = spark.sql(\"show partitions {0}\".format(df_name)).toPandas()\n",
    "    try:\n",
    "        dt_list = df.partition.apply(lambda x: [ele for ele in x.split('/') if ele[:3] == 'dt='][0][3:])\n",
    "        return sorted([ele for ele in dt_list if ele[0].isnumeric()])[-1]\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sale qtty data\n",
    "sales_df = spark.sql('''\n",
    "                         select\n",
    "                             item_sku_id,\n",
    "                             qtty_by_order_dt as sale_qtty,\n",
    "                             booking_flag,\n",
    "                             booking_pay_flag,\n",
    "                             presale_flag,\n",
    "                             presale_pay_flag,\n",
    "                             1 as on_shelf,\n",
    "                             0 as is_padded,\n",
    "                             dt as date\n",
    "                         from\n",
    "                             app.cid2_794_daily_sale\n",
    "                     ''')\n",
    "\n",
    "# instant promo data\n",
    "instant_df = spark.sql('''\n",
    "                             select\n",
    "                                 item_sku_id,\n",
    "                                 1 as instant_flag,\n",
    "                                 1 as expose_flag,\n",
    "                                 cast(instant_hour as int) as instant_hour,\n",
    "                                 instant_price,\n",
    "                                 dt as date\n",
    "                             from\n",
    "                                 app.jingpin_instant_price_uv_clean_test\n",
    "                             where\n",
    "                                 instant_channel = 2\n",
    "                                 and uv >= 50\n",
    "                         ''')\n",
    "\n",
    "# red price data\n",
    "price_df = spark.sql('''\n",
    "                         select \n",
    "                             sku_id as item_sku_id,\n",
    "                             max_time_price as redprice,\n",
    "                             dt as date\n",
    "                         from\n",
    "                             dev.self_sku_redprice_group\n",
    "                     ''')\n",
    "\n",
    "# nominal netprice\n",
    "nominal_price_df = spark.sql('''\n",
    "                                 select\n",
    "                                     sku_id as item_sku_id,\n",
    "                                     dsj as nominal_netprice,\n",
    "                                     dt as date\n",
    "                                 from\n",
    "                                     dev.dev_jingpin_sku_chan_dt_cc_stais_sh\n",
    "                                 where\n",
    "                                     channel = 0\n",
    "                             ''')\n",
    "\n",
    "# transaction data\n",
    "trans_df = spark.sql('''\n",
    "                         select\n",
    "                             item_sku_id,\n",
    "                             sum(after_prefr_amount - dq_pay_amount) * 1.0 / sum(sale_qtty) as netprice,\n",
    "                             dt as date\n",
    "                         from\n",
    "                             app.app_pa_transactions_d_self\n",
    "                         where\n",
    "                             dt >= '2018-01-01'\n",
    "                         group by \n",
    "                             item_sku_id, dt\n",
    "                     ''')\n",
    "\n",
    "max_dt = extract_max_dt('app.cid2_794_daily_sale')\n",
    "# sku-brand-cid3\n",
    "sku_info = spark.sql('''\n",
    "                         select\n",
    "                             sku_id as item_sku_id,\n",
    "                             brand_code,\n",
    "                             cid3\n",
    "                         from\n",
    "                             dev.self_sku_det_da\n",
    "                         where\n",
    "                             dt = '{0}'\n",
    "                             and cid1 = 737\n",
    "                             and cid2 = 794\n",
    "                             and item_valid_flag = 1 \n",
    "                             and sku_valid_flag = 1 \n",
    "                     '''.format(max_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "instant_df = instant_df.withColumn('instant_hour', 24 - F.col('instant_hour'))\n",
    "instant_df_nextday = instant_df.where((F.col('instant_flag') == 1) & (F.col('instant_hour') != 24)) \\\n",
    "                               .withColumn('date', F.date_add(F.col('date'), 1)) \\\n",
    "                               .withColumn('instant_hour_nextday', 24 - F.col('instant_hour')) \\\n",
    "                               .withColumnRenamed('instant_price', 'instant_price_nextday') \\\n",
    "                               .select('item_sku_id', 'instant_hour_nextday', 'instant_price_nextday', 'date')\n",
    "\n",
    "instant_df = instant_df.join(instant_df_nextday, on=['item_sku_id', 'date'], how='outer') \\\n",
    "                       .withColumn('instant_hour', F.when(F.isnull(F.col('instant_hour_nextday')), \n",
    "                                                          F.col('instant_hour')).otherwise(F.col('instant_hour_nextday'))) \\\n",
    "                       .withColumn('instant_flag', F.when(F.col('instant_hour') > 0, 1).otherwise(0)) \\\n",
    "                       .withColumn('instant_price', F.when(F.isnull(F.col('instant_hour_nextday')), \n",
    "                                                           F.col('instant_price')).otherwise(F.col('instant_price_nextday'))) \\\n",
    "                       .fillna({'expose_flag': 0}) \\\n",
    "                       .select('item_sku_id', 'instant_flag', 'expose_flag', 'instant_hour', 'instant_price', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sales_df.join(sku_info, on='item_sku_id', how='inner') \\\n",
    "             .join(instant_df, on=['item_sku_id', 'date'], how='left') \\\n",
    "             .join(price_df, on=['item_sku_id', 'date'], how='left') \\\n",
    "             .join(nominal_price_df, on=['item_sku_id', 'date'], how='left') \\\n",
    "             .join(trans_df, on=['item_sku_id', 'date'], how='left') \\\n",
    "             .fillna({'instant_flag': 0, 'expose_flag': 0, 'instant_hour': 0, \n",
    "                      'booking_pay_flag': 0, 'presale_flag': 0, 'presale_pay_flag': 0}) \\\n",
    "             .withColumn('instant_price', \n",
    "                         F.when(F.isnull(F.col('instant_price')), F.col('redprice')).otherwise(F.col('instant_price'))) \\\n",
    "             .withColumn('nominal_netprice',\n",
    "                         F.when(F.isnull(F.col('nominal_netprice')), F.col('redprice')).otherwise(F.col('nominal_netprice'))) \\\n",
    "             .withColumn('netprice',\n",
    "                         F.when(F.isnull(F.col('netprice')), F.col('redprice')).otherwise(F.col('netprice'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill off-shelf dates\n",
    "sales_date = data.groupby('item_sku_id').apply(lambda x:\n",
    "                                               pd.DataFrame(pd.date_range(x.date.min(), x.date.max()).astype(str),\n",
    "                                                            columns=['date'])).reset_index(level=0)\n",
    "data = data.merge(sales_date, on=['item_sku_id', 'date'], how='right')\n",
    "data.fillna({'booking_flag': 0, 'booking_pay_flag': 0, 'presale_flag': 0, 'presale_pay_flag': 0, \n",
    "             'on_shelf': 0, 'is_padded': 0, 'instant_flag': 0, 'expose_flag': 0, 'instant_hour': 0}, inplace=True)\n",
    "\n",
    "# transform instant_hour into one-hot\n",
    "# data['instant_hour'] = data.instant_hour.astype(int).astype(str).str.zfill(2)\n",
    "# data = pd.concat([data, pd.get_dummies(data.instant_hour, prefix='instant_hour', drop_first=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_forward_backward(series):\n",
    "    return series.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# fillna for each sku\n",
    "data = data.sort_values(['item_sku_id', 'date']).reset_index()\n",
    "cols_to_fill = ['brand_code', 'cid3', 'redprice']\n",
    "filled_df = data.groupby('item_sku_id').apply(lambda x: \n",
    "                        pd.concat([fillna_forward_backward(x[col]) for col in cols_to_fill], axis=1))\n",
    "data = pd.concat([data.drop(cols_to_fill, axis=1), filled_df], axis=1)\n",
    "data['instant_price'] = data.instant_price.fillna(data.redprice)\n",
    "data['nominal_netprice'] = data.nominal_netprice.fillna(data.redprice)\n",
    "data['netprice'] = data.netprice.fillna(data.redprice)\n",
    "data = data[\n",
    "    ['item_sku_id', 'date', 'sale_qtty', 'booking_flag', 'booking_pay_flag', 'presale_flag', 'presale_pay_flag', \n",
    "     'on_shelf', 'is_padded', 'brand_code', 'cid3', 'instant_flag', 'expose_flag', 'instant_hour', 'instant_price', \n",
    "     'redprice', 'nominal_netprice', 'netprice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cid2_794_sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
