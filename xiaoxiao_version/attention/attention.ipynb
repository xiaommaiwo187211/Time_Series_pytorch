{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from model import EncoderBahdanau, AttentionBahdanau, DecoderBahdanau, Seq2SeqBahdanau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from model import *\n",
    "from base import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(arr_list, device):\n",
    "    tensor_list = []\n",
    "    for arr in arr_list:\n",
    "        tensor_list.append(torch.tensor(arr, device=device, dtype=torch.float))\n",
    "    return tensor_list\n",
    "\n",
    "\n",
    "def train_epoch(model, input_seq_len, output_seq_len, batch_size, optimizer, criterion, clip, device):\n",
    "    # no need to use it here since there's no dropout or batchnorm\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss, iterations = 0, 0\n",
    "    for inputs, outputs, targets, start_points in generate_samples(Xtrain, ytrain, batch_size,\n",
    "                                                                   input_seq_len, output_seq_len):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, outputs, targets = to_tensor([inputs, outputs, targets], device)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        outputs = model(inputs, outputs, teacher_forcing_ratio=0.3)\n",
    "        if len(outputs) == 2:\n",
    "            outputs = outputs[0]\n",
    "#         import pdb; pdb.set_trace()\n",
    "        loss = criterion(outputs.view(-1), targets.view(-1))\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        iterations += 1\n",
    "\n",
    "    # average batch loss\n",
    "    # return epoch_loss / iterations\n",
    "\n",
    "    # mse (set reduction to sum)\n",
    "    total_num = output_seq_len * (len(Xtrain) - input_seq_len - output_seq_len + 1)\n",
    "    return epoch_loss / total_num\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if len(param.data.size()) == 1:\n",
    "            nn.init.normal_(param.data, 0, 1)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(param.data)\n",
    "\n",
    "\n",
    "def set_seed():\n",
    "    SEED = 12\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBahdanau(nn.Module):\n",
    "    def __init__(self, inputs_size, encoder_hidden_size, decoder_hidden_size):\n",
    "        super(EncoderBahdanau, self).__init__()\n",
    "        self.gru = nn.GRU(inputs_size, encoder_hidden_size, bidirectional=True)\n",
    "        self.linear = nn.Linear(encoder_hidden_size * 2, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (sequence, batch, feature)\n",
    "        # outputs: (sequence, batch, hidden * direction)\n",
    "        # hidden: (layer * direction, batch, hidden)\n",
    "\n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "\n",
    "        outputs, hidden = self.gru(inputs) # hidden:torch.Size([2, 256, 64]) INPUT_SEQ_LEN= 30; 256 = batch_size;  hidden_size =128\n",
    "        hidden = torch.tanh(self.linear(torch.cat((hidden[::2, :, :], hidden[1::2, :, :]), dim=2)))\n",
    "        return outputs, hidden  ### Output: torch.Size([30, 256, 128]); hidden: torch.Size([1, 256, 128])\n",
    "\n",
    "\n",
    "class AttentionBahdanau(nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n",
    "        super(AttentionBahdanau, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_hidden_size * 2 + decoder_hidden_size, decoder_hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(decoder_hidden_size), requires_grad=True)  # size = [128]\n",
    "\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # encoder outputs: (encoder_sequence, batch, encoder_hidden * direction)\n",
    "        # decoder hidden: (layer, batch, decoder_hidden)\n",
    "        # attention: (batch, encoder_sequence)\n",
    "        encoder_seq_len, batch_size, _ = encoder_outputs.size() ##  encoder_outputs : torch.Size([30, 256, 128])\n",
    "        decoder_hidden = decoder_hidden.repeat(encoder_seq_len, 1, 1) ## decoder_hidden : torch.Size([1, 256, 64]) 每次传一个time_step --> decoder_hidden :torch.Size([30, 256, 64])\n",
    "        # pass encoder outputs and previous decoder hidden through a multi-layer perceptron, this is called an alignment model\n",
    "        # e(i, j) = a(s(i-1), h(j))\n",
    "        # energy scores how well  the inputs around position j and the outputs around position i match\n",
    "        energy = torch.tanh(self.attn(torch.cat([encoder_outputs, decoder_hidden], dim=2))) ### energy : torch.Size([30, 256, 128])\n",
    "        # bmm requires batch first\n",
    "        energy = energy.permute(1, 2, 0)   ###[256, 128,30]  (batch, hidden, tiem_step)\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1) ## torch.Size([256, 1, 128])\n",
    "        attention = torch.softmax(torch.bmm(v, energy).squeeze(1), dim=1) #[256, 1, 128] * [256, 128,30] = [256, 1, 30].squeeze(1) = [256,30]\n",
    "        # soft_max(dim=0)对列进行softmax; dim=1为 行。 对每一个batch样本中，不同的time_step进行softmax。\n",
    "        return attention\n",
    "\n",
    "\n",
    "class DecoderBahdanau(nn.Module):\n",
    "    def __init__(self, inputs_size, encoder_hidden_size, decoder_hidden_size, attention):\n",
    "        super(DecoderBahdanau, self).__init__()\n",
    "        self.gru = nn.GRU(inputs_size + encoder_hidden_size * 2, decoder_hidden_size)    \n",
    "        ### decoder_input: (time_step, batch, features)(1, batch_size, inputs_size + encoder_hidden_size * 2)     [1, 256, 128] --> [1, 256, 128]\n",
    "        self.attention = attention  ### [256, 30]\n",
    "        self.linear = nn.Linear(encoder_hidden_size * 2 + decoder_hidden_size + inputs_size, 1)  ### [128 + 128 + 30, 1]\n",
    "\n",
    "    def forward(self, inputs, decoder_hidden, encoder_outputs):\n",
    "        # inputs: (1, batch, feature)\n",
    "        # decoder hidden: (1, batch, decoder_hidden)\n",
    "        # encoder outputs: (sequence, batch, encoder_hidden * direction)\n",
    "        attn_weights = self.attention(decoder_hidden, encoder_outputs)   ### [256,30]\n",
    "        # bmm requires batch first\n",
    "        attn_weights = attn_weights.unsqueeze(1)  ### [256,1,30]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) ### torch.Size([256, 30, 128])\n",
    "        import pdb; pdb.set_trace()        \n",
    "        attn_weighted = torch.bmm(attn_weights, encoder_outputs).permute(1, 0, 2) ###  [256,1,30]*([256, 30, 128])= [256,1,128].permute = [1,256,128] \n",
    "        output, hidden = self.gru(torch.cat([inputs, attn_weighted], dim=2), decoder_hidden) ### torch.Size([1, 256, 24]) + [1,256,128] = [1, 256, 128+24]\n",
    "        ### hidden: [1,256,128+24] --> [1,256,128], output: [1, 256, 128]\n",
    "        output = self.linear(torch.cat([inputs.squeeze(0), attn_weighted.squeeze(0), output.squeeze(0)], dim=1))\n",
    "\n",
    "\n",
    "        return hidden, output, attn_weights.squeeze(1)\n",
    "\n",
    "\n",
    "class Seq2SeqBahdanau(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqBahdanau, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, outputs, mean_, std_, teacher_forcing_ratio=0.5):\n",
    "        # inputs: (input_sequence, batch, feature)\n",
    "        # outputs: (output_sequence, batch, feature)\n",
    "        # targets: (output_sequence, batch, 1)\n",
    "        input_sequence_size = inputs.size(0)\n",
    "        output_sequence_size, batch_size, feature_size = outputs.size()\n",
    "        encoder_outputs, encoder_hidden = self.encoder(inputs)\n",
    "\n",
    "        decoder_input = torch.cat([inputs[-1:][:, :, :1], outputs[:1][:, :, 1:]], dim=2)   \n",
    "        ### inputs最后一个label,outputs所有的features\n",
    "        decoder_hidden = encoder_hidden ## encoder的最后一个h\n",
    "        decoder_outputs = torch.zeros(output_sequence_size, batch_size, 1).to(self.device)  ###生成所有output为0\n",
    "        attentions = torch.zeros(output_sequence_size, batch_size, input_sequence_size).to(self.device)\n",
    "        #### 初始化attentions [1,256,1] \n",
    "        for i in range(output_sequence_size):\n",
    "            decoder_hidden, decoder_output, attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs[i] = decoder_output\n",
    "            attentions[i] = attention\n",
    "\n",
    "            if i == output_sequence_size - 1:\n",
    "                break\n",
    "\n",
    "            if np.random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = torch.cat([outputs[i:i+1][:, :, :1], outputs[i+1:i+2][:, :, 1:]], dim=2)\n",
    "            else:\n",
    "                # no teacher forcing\n",
    "                decoder_output = (decoder_output - mean_) / std_\n",
    "                decoder_input = torch.cat([decoder_output.unsqueeze(0), outputs[i+1:i+2][:, :, 1:]], dim=2)\n",
    "        return decoder_outputs, attentions\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, FEATURE_SIZE, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.gru = nn.GRU(FEATURE_SIZE, ENCODER_HIDDEN_SIZE, 1, bidirectional=True)\n",
    "                           # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    " \n",
    "        self.linear = nn.Linear(ENCODER_HIDDEN_SIZE*2, DECODER_HIDDEN_SIZE)\n",
    "        ### 将encoder_hidden_size转化为decoder_hidden_size\n",
    "    def forward(self, encoder_inputs):\n",
    "        encoder_hideen_outputs, hidden = self.gru(encoder_inputs) ##encoder_hidden_size*2\n",
    "        decoder_hidden = torch.tanh(self.linear(torch.cat([hidden[::2,:,:],hidden[1::2,:,:]],dim=2)))\n",
    "        return encoder_hideen_outputs, decoder_hidden\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(ENCODER_HIDDEN_SIZE*2 + DECODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE)\n",
    "        self.v = nn.Parameter(torch.rand(DECODER_HIDDEN_SIZE), requires_grad = True)  # [10]\n",
    "    def forward(self, encoder_hidden_outputs, decoder_hidden):\n",
    "        encoder_time_step, batch_size, _ = encoder_hidden_outputs.size()\n",
    "        decoder_hidden_repeat = decoder_hidden.repeat(encoder_time_step, 1,1)\n",
    "        # encoder_hidden_outputs: [30, 256, DEcoder_hidden_size]\n",
    "        # decoder_hidden_repeat: [30, 256, DEcoder_hidden_size]\n",
    "        energy = torch.tanh(self.attn(torch.cat([encoder_hidden_outputs, decoder_hidden_repeat],dim=2)))  \n",
    "        # [encoder_time_step, batch, encoder_hidden*2] + [encoder_time_step, batch, decoder_hidden]\n",
    "        # self.v [decoder_hidden]\n",
    "        v = self.v.repeat(batch_size,1).unsqueeze(1)    ###[batch_size,1,decoder_hidden_size]\n",
    "        attention = torch.softmax(torch.bmm(v,energy.permute(1,2,0)).squeeze(1),dim=1) ### \n",
    "         ###[batch_size,1,decoder_hidden_size] * [batch_size, decoder_hidden, encoder_time_step] =[batch,1,encoder_time_step] -->[batch,encoder_time_step]\n",
    "        return attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE,ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.gru = nn.GRU(ENCODER_HIDDEN_SIZE*2 + INPUT_SIZE, DECODER_HIDDEN_SIZE)  ### 必须给decoder_hidden_size\n",
    "        self.attention = attention\n",
    "        self.linear = nn.Linear(INPUT_SIZE + ENCODER_HIDDEN_SIZE*2 + DECODER_HIDDEN_SIZE,1)\n",
    "    def forward(self,inputs ,encoder_hidden_outputs, decoder_hidden):\n",
    "        attn_weights = self.attention(encoder_hidden_outputs, decoder_hidden).unsqueeze(1) ### [batch,1, encoder_time_step]\n",
    "        encoder_hidden_outputs = encoder_hidden_outputs.permute(1, 0, 2) ### encoder_hidden_outputs: [time_step, batch, encoder_hidden_size*2].permute\n",
    "        attn_weighted = torch.bmm(attn_weights, encoder_hidden_outputs).permute(1, 0, 2)  \n",
    "        ### [batch,1, encoder_time_step]*[batch,encoder_time_step, encoder_hidden_size*2] =[batch, 1, encoder_hidden_size*2]\n",
    "        output, hidden = self.gru(torch.cat([inputs, attn_weighted], dim=2),decoder_hidden)     ###[ 1, 256, 24] ,   attn_weighted = [256,1,64*2]\n",
    "        ###此处不传decoder_hidden, 则默认decoder_hidden=0\n",
    "        output = self.linear(torch.cat([inputs.squeeze(0), attn_weighted.squeeze(0), output.squeeze(0)], dim=1))  \n",
    "        ### inputs: [time=1, batch, features], attn_weighted: [batch, 1, encoder_hidden_size*2], output: [time, batch, decoder_hidden]\n",
    "        return hidden, output, attn_weights.squeeze(1)\n",
    "        \n",
    "        \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder,decoder, teacher_forcing_ratio = 0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder    \n",
    "    def forward(self,inputs_batch, outputs_batch, teacher_forcing_ratio):\n",
    "        encoder_hidden_outputs, encoder_hidden = self.encoder(inputs_batch)\n",
    "\n",
    "        decoder_time_step, batch_size, decoder_feature_size = outputs_batch.size()\n",
    "        input_sequence_size = inputs_batch.size(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_inputs = torch.cat([inputs_batch[-1:][:,:,:1], outputs_batch[:1][:,:,1:]],dim=2)  ###需要保持维度：所以要用[-1:]\n",
    "        decoder_outputs = torch.zeros(decoder_time_step, batch_size, 1)\n",
    "        attentions = torch.zeros(decoder_time_step,batch_size, input_sequence_size)\n",
    "        for i in range(decoder_time_step):\n",
    "            decoder_hidden, decoder_output, attention = self.decoder(decoder_inputs, encoder_hidden_outputs, decoder_hidden)    \n",
    "            decoder_outputs[i] = decoder_output\n",
    "            attentions[i] = attention\n",
    "            if i == decoder_time_step - 1:\n",
    "                break\n",
    "            if np.random.random()< teacher_forcing_ratio:\n",
    "                decoder_inputs = torch.cat([outputs_batch[i:i+1][:,:,:1], outputs_batch[i+1:i+2][:,:,1:]],dim=2)\n",
    "            else:\n",
    "                decoder_inputs = torch.cat([decoder_output.unsqueeze(0), outputs_batch[i+1:i+2][:,:,1:]], dim=2)\n",
    "#         import pdb; pdb.set_trace()        \n",
    "        return decoder_outputs, attentions   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   decoder_inputs.size()\n",
    "# torch.Size([1, 256, 24])\n",
    "# (Pdb)  encoder_hidden_outputs.size()\n",
    "# torch.Size([30, 256, 128])\n",
    "# (Pdb)  decoder_hidden.size()\n",
    "# torch.Size([1, 256, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = pd.read_csv('PRSA_data_2010.1.1-2014.12.31.xls')\n",
    "\n",
    "data = create_features(data)\n",
    "# pm2.5列必须放在第一个\n",
    "FEATURE_COLS = ['pm2.5', 'year', 'TEMP']\n",
    "DATE_COLS = ['sin_week', 'cos_week', 'sin_hour', 'cos_hour', 'month_2', 'month_3', 'month_4', 'month_5',\n",
    "             'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n",
    "             'month_12', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
    "             'weekday_5', 'weekday_6']\n",
    "Xtrain, ytrain, Xtest, ytest, mean_, std_ = preprocessing(data, FEATURE_COLS, DATE_COLS)\n",
    "\n",
    "\n",
    "FEATURE_SIZE = len(FEATURE_COLS) + len(DATE_COLS)\n",
    "ENCODER_HIDDEN_SIZE = 64\n",
    "DECODER_HIDDEN_SIZE = 10\n",
    "LAYER_NUM = 2\n",
    "ENCODER_DROPOUT = 0.5\n",
    "DECODER_DROPOUT = 0.5\n",
    "encoder = Encoder(FEATURE_SIZE, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE)\n",
    "attn = Attention(ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE)\n",
    "decoder = Decoder(FEATURE_SIZE, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE, attn)\n",
    "seq2seq = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 41s\n",
      "\tTrain Loss: 1.345\n",
      "Epoch: 02 | Time: 1m 40s\n",
      "\tTrain Loss: 0.574\n",
      "Epoch: 03 | Time: 1m 41s\n",
      "\tTrain Loss: 0.501\n",
      "Epoch: 04 | Time: 1m 39s\n",
      "\tTrain Loss: 0.478\n",
      "Epoch: 05 | Time: 1m 39s\n",
      "\tTrain Loss: 0.470\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-66866b9a51ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     train_loss = train_epoch(seq2seq, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN,\n\u001b[0;32m---> 19\u001b[0;31m                              BATCH_SIZE, optimizer, criterion, CLIP, device)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-be9457de6f64>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, input_seq_len, output_seq_len, batch_size, optimizer, criterion, clip, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq2seq.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(seq2seq.parameters(), lr=0.1, nesterov=True, momentum=0.9)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "INPUT_SEQ_LEN = 30\n",
    "OUTPUT_SEQ_LEN = 14\n",
    "BATCH_SIZE = 256\n",
    "EPOCH_NUM = 10\n",
    "CLIP = 1\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    train_loss = train_epoch(seq2seq, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN,\n",
    "                             BATCH_SIZE, optimizer, criterion, CLIP, device)\n",
    "\n",
    "    set_seed()\n",
    "#     val_loss = evaluate(seq2seq, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN,\n",
    "#                         BATCH_SIZE, criterion, device)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print('Epoch: %s | Time: %sm %ss' % (str(epoch+1).zfill(2), epoch_mins, epoch_secs))\n",
    "    print('\\tTrain Loss: %.3f' % (train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pred_target(model, input_seq_len, output_seq_len, batch_size, criterion, device):\n",
    "    # no need to use it here since there's no dropout or batchnorm\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss, iterations = 0, 0\n",
    "    with torch.no_grad():\n",
    "        start_points_length = Xtest.shape[0] - input_seq_len - output_seq_len + 1\n",
    "        outputs_mat = torch.zeros(start_points_length, output_seq_len)\n",
    "        targets_mat = torch.zeros(start_points_length, output_seq_len)\n",
    "        attentions_mat = torch.zeros(start_points_length, output_seq_len, input_seq_len)\n",
    "        start_points_mat = torch.zeros(start_points_length, 1)\n",
    "        for inputs, outputs, targets, start_points in generate_samples(Xtest, ytest, batch_size,\n",
    "                                                                       input_seq_len, output_seq_len):\n",
    "            inputs, outputs, targets, start_points = to_tensor([inputs, outputs, targets, start_points], device)\n",
    "            # turn off teacher forcing\n",
    "            outputs, attentions = model(inputs, outputs, teacher_forcing_ratio=0)\n",
    "            outputs_mat[iterations*batch_size:(iterations+1)*batch_size] = torch.expm1(outputs[:, :, 0]).transpose(0, 1)\n",
    "            targets_mat[iterations*batch_size:(iterations+1)*batch_size] = torch.expm1(targets[:, :, 0]).transpose(0, 1)\n",
    "            attentions_mat[iterations*batch_size:(iterations+1)*batch_size] = attentions.permute(1, 0, 2)\n",
    "            start_points_mat[iterations*batch_size:(iterations+1)*batch_size] = start_points.view(-1, 1)\n",
    "            iterations += 1\n",
    "\n",
    "    return outputs_mat.numpy(), targets_mat.numpy(), attentions_mat.numpy(), start_points_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, l3, l4 =return_pred_target(seq2seq, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN, BATCH_SIZE, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(l1, columns=['l1','l2','l3','l4', 'l5', 'l6','l7','l8','l9','l10', 'l11', 'l12','l13', 'l14'])\n",
    "d2 = pd.DataFrame(l2, columns=['l1','l2','l3','l4', 'l5', 'l6','l7','l8','l9','l10', 'l11', 'l12','l13', 'l14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "d11 = pd.DataFrame(d1.apply(lambda x: np.sum(x),axis=0).values,columns=['prediction']).reset_index().rename(columns={'index': 'home'})\n",
    "d12 = pd.DataFrame(d2.apply(lambda x: np.sum(x),axis=0).values,columns=['real']).reset_index().rename(columns={'index': 'home'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f375f1546d8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = d11.plot(x='home',y='prediction', figsize=(12,4),legend='initial')\n",
    "d12.plot(x='home',y='real',figsize=(12,4),ax=ax,legend='after distinct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = tt.plot(x='dt',y='forecast_x', figsize=(12,4),legend='initial')\n",
    "tt.plot(x='dt',y='forecast_y',figsize=(12,4),ax=ax,legend='after distinct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_seq_len, output_seq_len, batch_size, criterion, device):\n",
    "    # no need to use it here since there's no dropout or batchnorm\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss, iterations = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, outputs, targets in generate_samples(Xtest, ytest, batch_size,\n",
    "                                                         input_seq_len, output_seq_len):\n",
    "            inputs, outputs, targets = to_tensor([inputs, outputs, targets], device)\n",
    "            # turn off teacher forcing\n",
    "            outputs = model(inputs, outputs, teacher_forcing_ratio=0)\n",
    "            loss = criterion(outputs.view(-1), targets.view(-1))\n",
    "            epoch_loss += loss\n",
    "            iterations += 1\n",
    "\n",
    "    # return epoch_loss / iterations\n",
    "    total_num = output_seq_len * (len(Xtest) - input_seq_len - output_seq_len + 1)\n",
    "    return epoch_loss / total_num\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
