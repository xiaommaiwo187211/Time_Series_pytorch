{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    year = pd.get_dummies(df['year'], drop_first=True, prefix='year')\n",
    "    month = pd.get_dummies(df['month'], drop_first=True, prefix='month')\n",
    "    df['date'] = df.year.astype(str) + '-' + df.month.astype(str).str.zfill(2) + '-' + df.day.astype(str).str.zfill(2)\n",
    "    df['week_day'] = pd.to_datetime(df.date).dt.dayofweek\n",
    "    week_day = pd.get_dummies(df['week_day'], drop_first=True, prefix='weekday')\n",
    "    df['week_num'] = pd.to_datetime(df.date).dt.weekofyear\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df.week_num / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df.week_num / 52)\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df.hour / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df.hour / 24)\n",
    "    return pd.concat([df, year, month, week_day], axis=1)\n",
    "\n",
    "data = pd.read_csv('PRSA_data_2010.1.1-2014.12.31.xls')\n",
    "\n",
    "data = create_features(data)\n",
    "# pm2.5列必须放在第一个\n",
    "FEATURE_COLS = ['pm2.5', 'year', 'TEMP']\n",
    "DATE_COLS = ['sin_week', 'cos_week', 'sin_hour', 'cos_hour', 'month_2', 'month_3', 'month_4', 'month_5',\n",
    "             'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n",
    "             'month_12', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
    "             'weekday_5', 'weekday_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 基础数据处理：1. (last: 31*24) = test; 2. 对 TEMP 进行均值化  ; 3. target 进行log\n",
    "\n",
    "df = data[FEATURE_COLS + DATE_COLS].copy()\n",
    "df = df.fillna(0)\n",
    "df['pm2.5'] = np.log1p(df['pm2.5'])  ## 近似于log1p\n",
    "\n",
    "Xtrain = df.iloc[:(-31*24),:].iloc[:-1,:]\n",
    "Ytrain = df.iloc[:(-31*24),:1].shift(-1).dropna().rename(columns= {'pm2.5':'target'})\n",
    "Xtest = df.iloc[(-31*24):,:].iloc[:-1,:]\n",
    "Ytest = df.iloc[(-31*24):,:1].shift(-1).dropna().rename(columns= {'pm2.5':'target'})\n",
    "\n",
    "for normal_features in set(FEATURE_COLS):\n",
    "    mean_ = Xtrain[normal_features].mean()\n",
    "    std_ = Xtrain[normal_features].std()\n",
    "    Xtrain[normal_features] = (Xtrain[normal_features] - mean_)/std_\n",
    "    Xtest[normal_features] = (Xtest[normal_features] - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, FEATURE_SIZE, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE):\n",
    "        super(encoder,self).__init__()\n",
    "        self.feature_size = FEATURE_SIZE\n",
    "        self.encoder_hidden_size = ENCODER_HIDDEN_SIZE\n",
    "        self.decoder_hidden_size = DECODER_HIDDEN_SIZE\n",
    "        self.gru = nn.GRU(FEATURE_SIZE, ENCODER_HIDDEN_SIZE, num_layers = 3, bidirectional = True)       \n",
    "        self.linear = nn.Linear(ENCODER_HIDDEN_SIZE * 2 * 3, DECODER_HIDDEN_SIZE)\n",
    "    def forward(self, ENCODER_INPUTS):\n",
    "        encoder_outputs, encoder_hidden = self.gru(ENCODER_INPUTS.float())   \n",
    "        hidden = torch.tanh(self.linear(torch.cat([encoder_hidden[::2,:,:], encoder_hidden[1::2,:,:]],dim=2).view(1,encoder_hidden.size(1),-1)))\n",
    "        return encoder_outputs, hidden\n",
    "\n",
    "### 生成不同encoder_hidden的alpha\n",
    "\n",
    "class attention(nn.Module):\n",
    "    def __init__(self, BATCH_SIZE, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE):\n",
    "        super(attention,self).__init__()\n",
    "        self.attn = nn.Linear(ENCODER_HIDDEN_SIZE*2 + DECODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE)\n",
    "        self.v = nn.Parameter(torch.rand(DECODER_HIDDEN_SIZE), requires_grad = True)   \n",
    "        \n",
    "    def forward(self, encoder_outputs, decoder_each_hidden):\n",
    "        encoder_outputs ### (time_step, batch, hidden_encoder)\n",
    "        decoder_each_hidden ###(time_step, batch, hidden_decoder)\n",
    "        encoder_time_step, batch_size, _ = encoder_outputs.size()\n",
    "        ### linear([hidden, output]) to hidden_size, [batch, hidden] * \n",
    "        v_repeat = self.v.repeat(encoder_time_step,1)  ### [time_step, decoder_hidden_size]\n",
    "        v_repeat = v_repeat.unsqueeze(1)  ### [time_step, 1, decoder_hidden_size]\n",
    "        decoder_each_hidden_repeat = decoder_each_hidden.squeeze(0).repeat(INPUT_SEQ_LEN, 1, 1)\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "        energy = torch.cat([encoder_outputs, decoder_each_hidden_repeat], dim=2)  ### (time_step, batch, 2* encoder_hidden + decoder_hidden)\n",
    "        energy_attn = self.attn(energy)  ### (time_step, batch, decoder_hidden)\n",
    "        \n",
    "        alpha_ = torch.softmax(torch.bmm(v_repeat, energy_attn.permute(0,2,1)).squeeze(1),dim=1)\n",
    "        #### [time_step, 1, decoder_hidden_size] bmm (time_step, decoder_hidden, batch) = [time_step, batch]\n",
    "        return alpha_     \n",
    "        \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, FEATURE_SIZE, DECODER_HIDDEN_SIZE,ENCODER_HIDDEN_SIZE, attention):\n",
    "        super(decoder,self).__init__()\n",
    "        self.gru = nn.GRU(FEATURE_SIZE+ENCODER_HIDDEN_SIZE*2, DECODER_HIDDEN_SIZE)\n",
    "        self.linear = nn.Linear(DECODER_HIDDEN_SIZE, 1)\n",
    "        self.attention = attention\n",
    "        \n",
    "    def forward(self, encoder_outputs, decoder_each_hidden, decoder_inputs):\n",
    "        alpha_ = self.attention(encoder_outputs, decoder_each_hidden)  \n",
    "        ### [time_step, batch] ### encoder_outputs:[time_step, batch, encoder_hidden*2]\n",
    "        ### attention needs to : [batch, decoder_hidden*2]\n",
    "        attention = torch.bmm(encoder_outputs.permute(1,2,0), alpha_.permute(1,0).unsqueeze(2))  \n",
    "        ### alpha_ = [time_step, batch] --> [batch, time] --> [batch,time,1]\n",
    "        ### encoder_outputs = [time, batch, hidden*2] --> [batch, time, hidden*2] --> [batch, hidden*2, time]\n",
    "        ### attention = [batch, hidden*2, 1]\n",
    "#         import pdb; pdb.set_trace()\n",
    "        decoder_input = torch.cat([attention.permute(2,0,1).float(), decoder_inputs.float()],dim=2) ### decoder_inputs = [1, batch, features]\n",
    "        output, hidden = self.gru(decoder_input, decoder_each_hidden)\n",
    "        output = self.linear(hidden)\n",
    "        return hidden, output, alpha_\n",
    "        \n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,ENCODER_INPUTS, decoder_inputs, teacher_forcing_ratio):\n",
    "        decoder_time_step, batch_size, _ = decoder_inputs.size()\n",
    "        encoder_time_size = ENCODER_INPUTS.size(0)\n",
    "        encoder_outputs, hidden = self.encoder(ENCODER_INPUTS)\n",
    "        decoder_each_hidden = hidden\n",
    "        outputs = torch.zeros(decoder_time_step, batch_size,1)\n",
    "        alpha_s = torch.zeros(decoder_time_step, encoder_time_size, batch_size)\n",
    "        for t in range(decoder_time_step):\n",
    "            hidden, output, alpha_ = self.decoder(encoder_outputs, decoder_each_hidden, decoder_inputs[t:t+1,:,:])\n",
    "            alpha_s[t:t+1,:,:] = alpha_\n",
    "            outputs[t:t+1,:,:] = output\n",
    "            \n",
    "            if t == decoder_time_step - 1:\n",
    "                break\n",
    "            \n",
    "            if np.random.random() < teacher_forcing_ratio:\n",
    "                decoder_inputs[t+1:t+2,:,:] = torch.cat([outputs[t:t+1,:,:].float(),decoder_inputs[t+1:t+2,:,1:].float()], dim=2)\n",
    "        return outputs, alpha_s\n",
    "        \n",
    "        \n",
    "# def evaluate(seq2seq, loader_test, INPUT_SEQ_LEN):\n",
    "#     with torch.no_grad():\n",
    "#         for times, (b_x, b_y) in enumerate(loader_train):\n",
    "#             model = seq2seq\n",
    "#             model.eval()\n",
    "#             outputs = model(b_x.permute(1,0,2)[:INPUT_SEQ_LEN,:,:], b_x.permute(1,0,2)[INPUT_SEQ_LEN:,:,:], 1)\n",
    "#         #     import pdb; pdb.set_trace()\n",
    "#             loss_sum = loss(outputs, b_y[:,INPUT_SEQ_LEN:,:].squeeze(2).float())   ### lack of exp()\n",
    "#     return outputs, loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构建time_step\n",
    "ENCODER_HIDDEN_SIZE = 64\n",
    "DECODER_HIDDEN_SIZE = 128\n",
    "LAYER_NUM = 2\n",
    "ENCODER_DROPOUT = 0.5\n",
    "DECODER_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "INPUT_SEQ_LEN = 30\n",
    "OUTPUT_SEQ_LEN = 14\n",
    "BATCH_SIZE = 256\n",
    "EPOCH_NUM = 100\n",
    "CLIP = 1\n",
    "\n",
    "FEATURE_SIZE = len(FEATURE_COLS) + len(DATE_COLS)\n",
    "\n",
    "### train集上数据处理\n",
    "\n",
    "sample_nums = (Xtrain.values.shape[0] - INPUT_SEQ_LEN - OUTPUT_SEQ_LEN -1)\n",
    "train_set_index = [list(range(i, i + INPUT_SEQ_LEN + OUTPUT_SEQ_LEN)) for i in range(sample_nums)]\n",
    "# for i in range()\n",
    "encoder_data = torch.tensor(np.take(Xtrain.values, train_set_index, axis =0))\n",
    "decoder_data = torch.tensor(np.take(Ytrain.values, train_set_index, axis =0))\n",
    "\n",
    "train = Data.TensorDataset(encoder_data, decoder_data)\n",
    "loader_train = Data.DataLoader(dataset = train,  batch_size = BATCH_SIZE, shuffle = True, num_workers = 4, drop_last= True )\n",
    "### x,y : [batch_size, time_step, features]\n",
    "\n",
    "def set_seed():\n",
    "    SEED = 12\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### next step: 1. for all batchs; 2. encoder中将所有time_step放里生成; 3. decoder中 for every time step,将其放入其中\n",
    "encoder = encoder(FEATURE_SIZE, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE)\n",
    "attention = attention(BATCH_SIZE, ENCODER_HIDDEN_SIZE, DECODER_HIDDEN_SIZE)\n",
    "decoder = decoder(FEATURE_SIZE, DECODER_HIDDEN_SIZE,ENCODER_HIDDEN_SIZE, attention)\n",
    "seq2seq = seq2seq(encoder, decoder)\n",
    "\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr = 0.00001)\n",
    "loss = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "def train_epoch(seq2seq, loader_train, INPUT_SEQ_LEN):\n",
    "    for times, (b_x, b_y) in enumerate(loader_train):\n",
    "        model = seq2seq\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs, alpha_s = model(b_x.permute(1,0,2)[:INPUT_SEQ_LEN,:,:], b_x.permute(1,0,2)[INPUT_SEQ_LEN:,:,:], 0.5)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        loss_sum = loss(outputs, b_y[:,INPUT_SEQ_LEN:,:].permute(1,0,2).float())   ### lack of exp()\n",
    "        optimizer.step()\n",
    "    return outputs, loss_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_nums_test = (Xtest.values.shape[0] - INPUT_SEQ_LEN - OUTPUT_SEQ_LEN -1)\n",
    "train_set_index_test = [list(range(i, i + INPUT_SEQ_LEN + OUTPUT_SEQ_LEN)) for i in range(sample_nums_test)]\n",
    "# for i in range()\n",
    "encoder_data_test = torch.tensor(np.take(Xtest.values, train_set_index_test, axis =0))\n",
    "decoder_data_test = torch.tensor(np.take(Ytest.values, train_set_index_test, axis =0))\n",
    "\n",
    "test = Data.TensorDataset(encoder_data_test, decoder_data_test)\n",
    "loader_test = Data.DataLoader(dataset = test,  batch_size = BATCH_SIZE, shuffle = True, num_workers = 4, drop_last= True )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPOCH = 2\n",
    "for epoch in range(EPOCH):\n",
    "    outputs, loss_sum = train_epoch(seq2seq, loader_train, INPUT_SEQ_LEN)\n",
    "    outputs_eval, loss_sum_eval = evaluate(seq2seq, loader_test, INPUT_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
